{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d999451-c8ec-40d2-bba7-4a44af6b80fb",
   "metadata": {},
   "source": [
    "# Standard Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bac05f-82b6-4756-97b5-2b166a090e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "url = \"https://s3-us-west-2.amazonaws.com/air-example-data/AnimalDetection/JPEGImages/2007_000063.jpg\"\n",
    "img = Image.open(requests.get(url, stream=True).raw)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6307d-9952-4f81-aa79-65a12cf2ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "\n",
    "weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "model = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.9)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb36b549-5a32-46f2-9ade-2d9780687fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.Compose([transforms.PILToTensor()])(img)\n",
    "preprocess = weights.transforms()\n",
    "batch = [preprocess(img)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5af97b2b-6afd-4bd6-ae52-fcbe56ffd406",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(batch)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8863d878-b61b-4ddf-ab04-e9d5978934a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "labels = [weights.meta[\"categories\"][i] for i in prediction[\"labels\"]]\n",
    "box = draw_bounding_boxes(img,\n",
    "                          boxes=prediction[\"boxes\"],\n",
    "                          labels=labels,\n",
    "                          colors=\"red\",\n",
    "                          width=4)\n",
    "im = to_pil_image(box.detach())\n",
    "display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9ba28-1d48-4681-9e26-7d0a01f62d43",
   "metadata": {},
   "source": [
    "# Scaling with Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b439697f-0800-4bee-83ec-62cabb2a1cc7",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a52a4e-9cc7-4a2d-beef-3e235a252d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.init(_temp_dir='/taiga/mohanar2/raytmp', ignore_reinit_error=True)\n",
    "ds = ray.data.read_images(\"./AnimalDetection\")\n",
    "display(ds.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00876b44-8fc2-44e2-b199-473bd880584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use stateful transforms to load model weights only once\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import (FasterRCNN_ResNet50_FPN_V2_Weights,\n",
    "                                          fasterrcnn_resnet50_fpn_v2)\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def preprocess_image(data: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "    preprocessor = transforms.Compose(\n",
    "        [transforms.ToTensor(), weights.transforms()]\n",
    "    )\n",
    "    return {\n",
    "        \"image\": data[\"image\"],\n",
    "        \"transformed\": preprocessor(data[\"image\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f25b8df-0f76-473d-81ad-31bf0968caf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2d7bb-d21c-4917-a789-3a6520ad3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_batch = ds.take_batch(batch_size=3)\n",
    "display(single_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47054675-d07f-4d6b-9833-d77ed28612fc",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36206bc8-e062-42a2-8997-38a7c84ca144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetectionModel:\n",
    "    def __init__(self):\n",
    "        # Define the model loading and initialization code in `__init__`.\n",
    "        self.weights = FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT\n",
    "        self.model = fasterrcnn_resnet50_fpn_v2(\n",
    "            weights=self.weights,\n",
    "            box_score_thresh=0.9,\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            # Move the model to GPU if it's available.\n",
    "            self.model = self.model.cuda()\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, input_batch: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        # Define the per-batch inference code in `__call__`.\n",
    "        batch = [torch.from_numpy(image) for image in input_batch[\"transformed\"]]\n",
    "        if torch.cuda.is_available():\n",
    "            # Move the data to GPU if it's available.\n",
    "            batch = [image.cuda() for image in batch]\n",
    "        predictions = self.model(batch)\n",
    "        return {\n",
    "            \"image\": input_batch[\"image\"],\n",
    "            \"labels\": [pred[\"labels\"].detach().cpu().numpy() for pred in predictions],\n",
    "            \"boxes\": [pred[\"boxes\"].detach().cpu().numpy() for pred in predictions],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44b363ec-a82f-45cc-b83e-53a914ec4e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map_batches(\n",
    "    ObjectDetectionModel,\n",
    "    # Use 1 GPUs. Change this number based on the number of GPUs in your cluster.\n",
    "    concurrency=1,\n",
    "    batch_size=4,  # Use the largest batch size that can fit in GPU memory.\n",
    "    # Specify 1 GPU per model replica. Remove this if you are doing CPU inference.\n",
    "    num_gpus=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5df5c7-a651-4515-ae7a-5b8aec5cd754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import convert_image_dtype, to_tensor\n",
    "count = 0\n",
    "batch = ds.take_batch(batch_size=30)\n",
    "for image, labels, boxes in zip(batch[\"image\"], batch[\"labels\"], batch[\"boxes\"]):\n",
    "    image = convert_image_dtype(to_tensor(image), torch.uint8)\n",
    "    labels = [weights.meta[\"categories\"][i] for i in labels]\n",
    "    boxes = torch.from_numpy(boxes)\n",
    "    img = to_pil_image(draw_bounding_boxes(\n",
    "        image,\n",
    "        boxes,\n",
    "        labels=labels,\n",
    "        colors=\"red\",\n",
    "        width=4,\n",
    "    ))\n",
    "    display(img)\n",
    "    count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3df792-ff07-487c-96f5-32b2f9aad3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
